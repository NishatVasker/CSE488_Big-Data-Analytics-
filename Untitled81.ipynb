{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkfUE94efLcs",
        "outputId": "167e5333-9fa8-4695-93fb-3def281039d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mrjob\n",
            "  Downloading mrjob-0.7.4-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from mrjob) (6.0.2)\n",
            "Downloading mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.6/439.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mrjob\n",
            "Successfully installed mrjob-0.7.4\n"
          ]
        }
      ],
      "source": [
        "pip install mrjob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from mrjob.job import MRJob\n",
        "\n",
        "# Step 1: Load and Prepare Dataset\n",
        "file_path = \"/content/retail_data.xlsx\"  # Adjust this path as needed\n",
        "data = pd.ExcelFile(file_path)\n",
        "data_2010_2011 = data.parse(\"Year 2010-2011\")\n",
        "\n"
      ],
      "metadata": {
        "id": "WnmH2PIyfMIU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Cleaning\n",
        "data_cleaned = data_2010_2011.dropna().copy()\n",
        "data_cleaned.loc[:, 'Invoice'] = data_cleaned['Invoice'].astype(str)\n",
        "data_cleaned.loc[:, 'Description'] = data_cleaned['Description'].str.strip()\n",
        "\n",
        "# Group transactions by Invoice\n",
        "transactions = data_cleaned.groupby(\"Invoice\")[\"Description\"].apply(list)\n",
        "\n",
        "# Save transactions to a text file\n",
        "transactions_file = \"transactions.txt\"\n",
        "with open(transactions_file, \"w\") as f:\n",
        "    for transaction in transactions:\n",
        "        f.write(\",\".join(transaction) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "R8ReQF_DfMKm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define the MRJob class for Frequent Itemsets Mining\n",
        "class MRFrequentItemsets(MRJob):\n",
        "    def configure_args(self):\n",
        "        super(MRFrequentItemsets, self).configure_args()\n",
        "        self.add_passthru_arg('--min-support', type=int, default=2, help='Minimum support threshold')\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        items = line.strip().split(',')\n",
        "        for itemset in combinations(items, 2):  # Generate pairs\n",
        "            yield itemset, 1\n",
        "\n",
        "    def combiner(self, itemset, counts):\n",
        "        yield itemset, sum(counts)\n",
        "\n",
        "    def reducer(self, itemset, counts):\n",
        "        total_count = sum(counts)\n",
        "        if total_count >= self.options.min_support:\n",
        "            yield itemset, total_count\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WToTTlJ-fMMf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('frequent_itemsets_mr.py', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "from mrjob.job import MRJob\n",
        "from itertools import combinations\n",
        "\n",
        "class MRFrequentItemsets(MRJob):\n",
        "    def configure_args(self):\n",
        "        super(MRFrequentItemsets, self).configure_args()\n",
        "        self.add_passthru_arg('--min-support', type=int, default=2, help='Minimum support threshold')\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        items = line.strip().split(',')\n",
        "        for itemset in combinations(items, 2):  # Generate pairs\n",
        "            yield itemset, 1\n",
        "\n",
        "    def combiner(self, itemset, counts):\n",
        "        yield itemset, sum(counts)\n",
        "\n",
        "    def reducer(self, itemset, counts):\n",
        "        total_count = sum(counts)\n",
        "        if total_count >= self.options.min_support:\n",
        "            yield itemset, total_count\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRFrequentItemsets.run()\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "bcHDRmvkfMOO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare transactions data\n",
        "transactions = [\n",
        "    ['milk', 'bread', 'butter'],\n",
        "    ['bread', 'butter', 'jam'],\n",
        "    ['milk', 'bread'],\n",
        "    ['milk', 'butter'],\n",
        "    ['bread', 'butter'],\n",
        "]\n",
        "\n",
        "# Save transactions to a file\n",
        "with open('transactions.txt', 'w') as f:\n",
        "    for transaction in transactions:\n",
        "        f.write(','.join(transaction) + '\\n')\n"
      ],
      "metadata": {
        "id": "v5XAUx4AfMPz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python frequent_itemsets_mr.py transactions.txt --min-support 2 > output.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhAtQBnmltxx",
        "outputId": "921d1fe2-b9ed-4f5f-c091-c597c63d6ef1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/frequent_itemsets_mr.root.20241120.172030.526101\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/frequent_itemsets_mr.root.20241120.172030.526101/output\n",
            "Streaming final output from /tmp/frequent_itemsets_mr.root.20241120.172030.526101/output...\n",
            "Removing temp directory /tmp/frequent_itemsets_mr.root.20241120.172030.526101...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display frequent itemsets\n",
        "with open('output.txt', 'r') as f:\n",
        "    print(f.read())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0jKqVDgltuK",
        "outputId": "5755c7b7-27f1-4a26-a49f-4b190567818e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"milk\", \"bread\"]\t2\n",
            "[\"bread\", \"butter\"]\t3\n",
            "[\"milk\", \"butter\"]\t2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Parse frequent itemsets from the output file\n",
        "frequent_itemsets = []\n",
        "with open('output.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        # Remove leading/trailing whitespace and check for empty lines\n",
        "        line = line.strip()\n",
        "        if not line:  # Skip empty lines\n",
        "            continue\n",
        "\n",
        "        # Split the line into itemset and count using a tab delimiter\n",
        "        parts = line.rsplit('\\t', 1)\n",
        "        if len(parts) != 2:  # Skip malformed lines\n",
        "            print(f\"Skipping malformed line: {line}\")\n",
        "            continue\n",
        "\n",
        "        itemset_str, count_str = parts[0], parts[1]\n",
        "\n",
        "        # Parse itemset: Remove brackets and split items\n",
        "        itemset = itemset_str.strip('[]').replace('\"', '').split(', ')\n",
        "        itemset = [item.strip() for item in itemset if item]  # Clean up item names\n",
        "        try:\n",
        "            count = int(count_str)  # Convert count to integer\n",
        "        except ValueError:\n",
        "            print(f\"Skipping line with invalid count: {line}\")\n",
        "            continue\n",
        "\n",
        "        frequent_itemsets.append((set(itemset), count))\n",
        "\n",
        "# Add singleton itemsets to frequent_itemsets\n",
        "singleton_itemsets = {}\n",
        "for itemset, count in frequent_itemsets:\n",
        "    for item in itemset:\n",
        "        if frozenset([item]) in singleton_itemsets:\n",
        "            singleton_itemsets[frozenset([item])] += count\n",
        "        else:\n",
        "            singleton_itemsets[frozenset([item])] = count\n",
        "\n",
        "# Add singletons to the frequent_itemsets list\n",
        "for itemset, count in singleton_itemsets.items():\n",
        "    frequent_itemsets.append((set(itemset), count))\n",
        "\n",
        "print(\"Frequent Itemsets Parsed (including singletons):\")\n",
        "for itemset, count in frequent_itemsets:\n",
        "    print(f\"Itemset: {itemset}, Count: {count}\")\n",
        "\n",
        "# Step 2: Generate Association Rules\n",
        "def generate_association_rules(frequent_itemsets, min_confidence=0.4):  # Lowered confidence threshold\n",
        "    rules = []\n",
        "    print(\"Generating Rules...\")\n",
        "    for itemset, support in frequent_itemsets:\n",
        "        if len(itemset) > 1:  # Only generate rules for itemsets with 2 or more items\n",
        "            for item in itemset:\n",
        "                antecedent = itemset - {item}\n",
        "                consequent = {item}\n",
        "                antecedent_support = next(\n",
        "                    (sup for items, sup in frequent_itemsets if items == antecedent), 0\n",
        "                )\n",
        "                print(f\"Analyzing: Itemset={itemset}, Antecedent={antecedent}, Consequent={consequent}, Support={support}, Antecedent Support={antecedent_support}\")\n",
        "                if antecedent_support > 0:  # Avoid division by zero\n",
        "                    confidence = support / antecedent_support\n",
        "                    print(f\"Confidence for rule {antecedent} -> {consequent}: {confidence:.2f}\")\n",
        "                    if confidence >= min_confidence:\n",
        "                        rules.append((antecedent, consequent, confidence))\n",
        "    return rules\n",
        "\n",
        "# Generate rules with a confidence threshold of 0.4\n",
        "association_rules = generate_association_rules(frequent_itemsets, min_confidence=0.4)\n",
        "\n",
        "# Step 3: Display Association Rules\n",
        "print(\"Association Rules:\")\n",
        "if not association_rules:\n",
        "    print(\"No association rules generated.\")\n",
        "else:\n",
        "    for antecedent, consequent, confidence in association_rules:\n",
        "        print(f\"Rule: {antecedent} -> {consequent} (Confidence: {confidence:.2f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPTKIogIltrj",
        "outputId": "46896ca0-10e9-4c1b-9e12-f8b1b3c03320"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Itemsets Parsed (including singletons):\n",
            "Itemset: {'milk', 'bread'}, Count: 2\n",
            "Itemset: {'bread', 'butter'}, Count: 3\n",
            "Itemset: {'milk', 'butter'}, Count: 2\n",
            "Itemset: {'milk'}, Count: 4\n",
            "Itemset: {'bread'}, Count: 5\n",
            "Itemset: {'butter'}, Count: 5\n",
            "Generating Rules...\n",
            "Analyzing: Itemset={'milk', 'bread'}, Antecedent={'bread'}, Consequent={'milk'}, Support=2, Antecedent Support=5\n",
            "Confidence for rule {'bread'} -> {'milk'}: 0.40\n",
            "Analyzing: Itemset={'milk', 'bread'}, Antecedent={'milk'}, Consequent={'bread'}, Support=2, Antecedent Support=4\n",
            "Confidence for rule {'milk'} -> {'bread'}: 0.50\n",
            "Analyzing: Itemset={'bread', 'butter'}, Antecedent={'butter'}, Consequent={'bread'}, Support=3, Antecedent Support=5\n",
            "Confidence for rule {'butter'} -> {'bread'}: 0.60\n",
            "Analyzing: Itemset={'bread', 'butter'}, Antecedent={'bread'}, Consequent={'butter'}, Support=3, Antecedent Support=5\n",
            "Confidence for rule {'bread'} -> {'butter'}: 0.60\n",
            "Analyzing: Itemset={'milk', 'butter'}, Antecedent={'butter'}, Consequent={'milk'}, Support=2, Antecedent Support=5\n",
            "Confidence for rule {'butter'} -> {'milk'}: 0.40\n",
            "Analyzing: Itemset={'milk', 'butter'}, Antecedent={'milk'}, Consequent={'butter'}, Support=2, Antecedent Support=4\n",
            "Confidence for rule {'milk'} -> {'butter'}: 0.50\n",
            "Association Rules:\n",
            "Rule: {'bread'} -> {'milk'} (Confidence: 0.40)\n",
            "Rule: {'milk'} -> {'bread'} (Confidence: 0.50)\n",
            "Rule: {'butter'} -> {'bread'} (Confidence: 0.60)\n",
            "Rule: {'bread'} -> {'butter'} (Confidence: 0.60)\n",
            "Rule: {'butter'} -> {'milk'} (Confidence: 0.40)\n",
            "Rule: {'milk'} -> {'butter'} (Confidence: 0.50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eVP4eRaQltpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I17Lm_YPltnL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}